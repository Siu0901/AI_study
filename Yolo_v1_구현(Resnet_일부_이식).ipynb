{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOsjP33hjUmhQy1Sn9mv348",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siu0901/AI_study/blob/main/Yolo_v1_%EA%B5%AC%ED%98%84(Resnet_%EC%9D%BC%EB%B6%80_%EC%9D%B4%EC%8B%9D).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AqBINK6pBro6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import VOCDetection\n",
        "import numpy as np\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 8\n",
        "WEIGHT_DECAY = 0\n",
        "EPOCHS = 10\n",
        "NUM_WORKERS = 2\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = False"
      ],
      "metadata": {
        "id": "X1VUHqCrXTti"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOC_CLASSES = [\n",
        "    \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\",\n",
        "    \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n",
        "    \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
        "]"
      ],
      "metadata": {
        "id": "I7V7qD4zLnxV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 매우매우 힘들었던 데이터 가져오기\n",
        "# 제미나이랑 입씨름 끝에 캐글에서 가져옴\n",
        "import kagglehub\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# 1. data 폴더 초기화\n",
        "if os.path.exists('./data'):\n",
        "    shutil.rmtree('./data')\n",
        "os.makedirs('./data/VOCdevkit', exist_ok=True)\n",
        "\n",
        "# 2. 데이터 경로 확인 (다운로드)\n",
        "print(\"경로 확인 중...\")\n",
        "path = kagglehub.dataset_download(\"huanghanchina/pascal-voc-2012\")\n",
        "print(f\"원본 경로: {path}\")\n",
        "\n",
        "# 3. 소스 경로 찾기 (VOC2012 폴더 위치 찾기)\n",
        "# 캐글 데이터셋 안에 VOC2012가 어디 박혀있는지 확인\n",
        "src_path = os.path.join(path, 'VOC2012')\n",
        "if not os.path.exists(src_path):\n",
        "    # 만약 VOCdevkit 안에 들어있다면\n",
        "    if os.path.exists(os.path.join(path, 'VOCdevkit', 'VOC2012')):\n",
        "        src_path = os.path.join(path, 'VOCdevkit', 'VOC2012')\n",
        "    else:\n",
        "        # 그것도 아니면 그냥 path 자체가 데이터임\n",
        "        src_path = path\n",
        "\n",
        "print(f\"찾은 소스 경로: {src_path}\")\n",
        "\n",
        "# 4. 복사 시작 (shutil.copytree 사용)\n",
        "# Move(이동)가 아니라 Copy(복사)를 해야 에러가 안 납니다!\n",
        "dst_path = './data/VOCdevkit/VOC2012'\n",
        "\n",
        "print(\"데이터 복사 중... (잠시만 기다리세요)\")\n",
        "shutil.copytree(src_path, dst_path)\n",
        "\n",
        "print(\"\\n[성공] 데이터 준비 완료!\")\n",
        "print(f\"데이터 위치: {dst_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kl3xm2YWmn7W",
        "outputId": "fb1f7e3f-8fe9-495f-bcdd-5ef7cf10f4fa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "경로 확인 중...\n",
            "Using Colab cache for faster access to the 'pascal-voc-2012' dataset.\n",
            "원본 경로: /kaggle/input/pascal-voc-2012\n",
            "찾은 소스 경로: /kaggle/input/pascal-voc-2012/VOC2012\n",
            "데이터 복사 중... (잠시만 기다리세요)\n",
            "\n",
            "[성공] 데이터 준비 완료!\n",
            "데이터 위치: ./data/VOCdevkit/VOC2012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 정제\n",
        "class VOCDataset(VOCDetection):\n",
        "    def __init__(self, root, year, image_set, download=False, S=7, B=2, C=20, transform=None):\n",
        "        super().__init__(root, year=year, image_set=image_set, download=download)\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = super().__getitem__(index)\n",
        "\n",
        "        # 이미지 크기 변경 전 정보 저장\n",
        "        original_width, original_height = img.size\n",
        "\n",
        "        # 이미지 리사이즈 및 텐서 변환\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # 정답 텐서 7x7x30 만들기\n",
        "        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
        "\n",
        "        # XML에서 객체 정보 가져오기\n",
        "        objects = target['annotation']['object']\n",
        "        if not isinstance(objects, list):\n",
        "            objects = [objects]\n",
        "\n",
        "        for obj in objects:\n",
        "            class_name = obj['name']\n",
        "            if class_name not in VOC_CLASSES:\n",
        "                continue\n",
        "\n",
        "            class_idx = VOC_CLASSES.index(class_name)\n",
        "            bndbox = obj['bndbox']\n",
        "\n",
        "            # 원본 크기 기준으로 좌표 가져오기\n",
        "            xmin = float(bndbox['xmin'])\n",
        "            ymin = float(bndbox['ymin'])\n",
        "            xmax = float(bndbox['xmax'])\n",
        "            ymax = float(bndbox['ymax'])\n",
        "\n",
        "            # 448x448 크기 맞춰 좌표 스케일링\n",
        "            scale_x = 448 / original_width\n",
        "            scale_y = 448 / original_height\n",
        "\n",
        "            x = (xmin * scale_x + xmax * scale_x) / 2\n",
        "            y = (ymin * scale_y + ymax * scale_y) / 2\n",
        "            w = (xmax * scale_x - xmin * scale_x)\n",
        "            h = (ymax * scale_y - ymin * scale_y)\n",
        "\n",
        "            # 0~1 사이 값으로 정규화\n",
        "            x /= 448\n",
        "            y /= 448\n",
        "            w /= 448\n",
        "            h /= 448\n",
        "\n",
        "            # 그리드 셀 인덱스 계산\n",
        "            i, j = int(self.S * y), int(self.S * x)\n",
        "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
        "\n",
        "            # 해당 셀에 아직 객체가 없으면 할당\n",
        "            if label_matrix[i, j, 20] == 0:\n",
        "                label_matrix[i, j, 20] = 1 # Confidence\n",
        "                label_matrix[i, j, 21:25] = torch.tensor([x_cell, y_cell, w, h])\n",
        "                label_matrix[i, j, class_idx] = 1\n",
        "\n",
        "        return img, label_matrix"
      ],
      "metadata": {
        "id": "O_ykiRDHk1aC"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((448, 448)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = VOCDataset(\n",
        "    root='./data',\n",
        "    year='2012',\n",
        "    image_set='trainval',\n",
        "    download=False,\n",
        "    transform=transform\n",
        ")\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "print(f\"전체 데이터: {len(dataset)}장\")\n",
        "print(f\"-> 학습용(Train): {len(train_dataset)}장\")\n",
        "print(f\"-> 테스트용(Test): {len(test_dataset)}장\")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    collate_fn=None\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY,\n",
        "    drop_last=False\n",
        ")"
      ],
      "metadata": {
        "id": "PLDDOf_WcqpP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85c0db8d-bfb1-4e52-fe81-5bd7f6c66a23"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 데이터: 11540장\n",
            "-> 학습용(Train): 9232장\n",
            "-> 테스트용(Test): 2308장\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "욜로 다크넷 구현 했는데 이거 이미지넷으로 선행 학습 돌려야 된다해서\n",
        "\n",
        "저 부분을 resnet50 가져와서 바꿔 끼고 마지막 벡본 부분만 조금 수정해서 코드 갈아끼는 걸로 바꿈"
      ],
      "metadata": {
        "id": "JEC-FLZ1NMSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class DarkNet(nn.Module):\n",
        "#     def __init__(self):\n",
        "#       super(DarkNet, self).__init__()\n",
        "\n",
        "#       self.feature = nn.Sequential(\n",
        "#           nn.Conv2d(3,64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "#           nn.BatchNorm2d(64),\n",
        "#           nn.LeakyReLU(0.1),\n",
        "#           nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "#           nn.Conv2d(64,192, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "#           nn.BatchNorm2d(192),\n",
        "#           nn.LeakyReLU(0.1),\n",
        "#           nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "#           nn.Conv2d(192,128, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "#           nn.BatchNorm2d(128),\n",
        "#           nn.LeakyReLU(0.1),\n",
        "#           nn.Conv2d(128,256, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "#           nn.BatchNorm2d(256),\n",
        "#           nn.LeakyReLU(0.1),\n",
        "#           nn.Conv2d(256,256, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "#           nn.BatchNorm2d(256),\n",
        "#           nn.LeakyReLU(0.1),\n",
        "#           nn.Conv2d(256,512, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "#           nn.BatchNorm2d(512),\n",
        "#           nn.LeakyReLU(0.1),\n",
        "#           nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "#           nn.Conv2d(512,256, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "#           nn.BatchNorm2d(256),\n",
        "#           nn.LeakyReLU(0.1),\n",
        "#           nn.Conv2d(256,512, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "#           nn.BatchNorm2d(512),\n",
        "#           nn.LeakyReLU(0.1),\n",
        "#           nn.Conv2d(512,256, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "#           nn.BatchNorm2d(256),\n",
        "#           nn.LeakyReLU(0.1),\n",
        "#           nn.Conv2d(256,512, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "#           nn.BatchNorm2d(512),\n",
        "#           nn.LeakyReLU(0.1),\n",
        "#           nn.Conv2d(512,256, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "#           nn.BatchNorm2d(256),\n",
        "#           nn.LeakyReLU(0.1),\n",
        "#           nn.Conv2d(256,512, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "#           nn.BatchNorm2d(512),\n",
        "#           nn.LeakyReLU(0.1),\n",
        "#           nn.Conv2d(512,256, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "#           nn.BatchNorm2d(256),\n",
        "#           nn.LeakyReLU(0.1),\n",
        "#           nn.Conv2d(256,512, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "#           nn.BatchNorm2d(512),\n",
        "#           nn.LeakyReLU(0.1),\n",
        "\n",
        "#           nn.Conv2d(512,512, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "#           nn.BatchNorm2d(512),\n",
        "#           nn.LeakyReLU(0.1),\n",
        "#           nn.Conv2d(512,1024, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "#           nn.BatchNorm2d(1024),\n",
        "#           nn.LeakyReLU(0.1),\n",
        "#           nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "#           nn.Conv2d(1024,512, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "#           nn.BatchNorm2d(512),\n",
        "#           nn.LeakyReLU(0.1),\n",
        "#           nn.Conv2d(512,1024, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "#           nn.BatchNorm2d(1024),\n",
        "#           nn.LeakyReLU(0.1),\n",
        "#           nn.Conv2d(1024,512, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "#           nn.BatchNorm2d(512),\n",
        "#           nn.LeakyReLU(0.1),\n",
        "#           nn.Conv2d(512,1024, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "#           nn.BatchNorm2d(1024),\n",
        "#           nn.LeakyReLU(0.1),\n",
        "#           nn.Conv2d(1024,1024, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "#           nn.BatchNorm2d(1024),\n",
        "#           nn.LeakyReLU(0.1),\n",
        "#           nn.Conv2d(1024,1024, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "#           nn.BatchNorm2d(1024),\n",
        "#           nn.LeakyReLU(0.1),\n",
        "\n",
        "#           nn.Conv2d(1024,1024, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "#           nn.BatchNorm2d(1024),\n",
        "#           nn.LeakyReLU(0.1),\n",
        "#           nn.Conv2d(1024,1024, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "#           nn.BatchNorm2d(1024),\n",
        "#           nn.LeakyReLU(0.1),\n",
        "#       )\n",
        "#       self.full_con = nn.Sequential(\n",
        "#           nn.Flatten(),\n",
        "#           nn.Linear(7*7*1024,4096),\n",
        "#           nn.LeakyReLU(0.1),\n",
        "          # nn.Linear(4096,7*7*30),\n",
        "          # nn.Unflatten(1,(7,7,30))\n",
        "#       )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#       x = self.feature(x)\n",
        "#       x = self.full_con(x)\n",
        "#       return x\n",
        "import torchvision.models as models\n",
        "\n",
        "class Yolov1_resnet50(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Yolov1_resnet50, self).__init__()\n",
        "\n",
        "        self.backbone = models.resnet50(weights='DEFAULT')\n",
        "        # resnet 뒷부분 fc 자르기 (2048, 14, 14) 크기로 나옴\n",
        "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
        "\n",
        "        self.feature = nn.Sequential(\n",
        "            nn.Conv2d(2048, 1024, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.1),\n",
        "\n",
        "            # 크기 14->7로 줄이기 (stride 2로 설정했음)\n",
        "            nn.Conv2d(1024, 1024, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.1),\n",
        "\n",
        "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1,bias=False),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.1),\n",
        "\n",
        "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1,bias=False),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.1),\n",
        "        )\n",
        "\n",
        "        self.full_con = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(7*7*1024,4096),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(4096,7*7*30),\n",
        "            nn.Unflatten(1,(7,7,30))\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.feature(x)\n",
        "        x = self.full_con(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "l33vRsEKKgtS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class YoloLoss(nn.Module):\n",
        "    def __init__(self, S=7, B=2, C=20):\n",
        "        super(YoloLoss, self).__init__()\n",
        "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
        "        self.S, self.B, self.C = S, B, C\n",
        "        self.lambda_noobj = 0.5\n",
        "        self.lambda_coord = 5\n",
        "\n",
        "    def compute_iou(self, box1, box2):\n",
        "        b1_x1, b1_y1 = box1[..., 0] - box1[..., 2] / 2, box1[..., 1] - box1[..., 3] / 2\n",
        "        b1_x2, b1_y2 = box1[..., 0] + box1[..., 2] / 2, box1[..., 1] + box1[..., 3] / 2\n",
        "        b2_x1, b2_y1 = box2[..., 0] - box2[..., 2] / 2, box2[..., 1] - box2[..., 3] / 2\n",
        "        b2_x2, b2_y2 = box2[..., 0] + box2[..., 2] / 2, box2[..., 1] + box2[..., 3] / 2\n",
        "\n",
        "        inter_x1 = torch.max(b1_x1, b2_x1)\n",
        "        inter_y1 = torch.max(b1_y1, b2_y1)\n",
        "        inter_x2 = torch.min(b1_x2, b2_x2)\n",
        "        inter_y2 = torch.min(b1_y2, b2_y2)\n",
        "\n",
        "        inter_area = (inter_x2 - inter_x1).clamp(0) * (inter_y2 - inter_y1).clamp(0)\n",
        "        box1_area = abs((b1_x2 - b1_x1) * (b1_y2 - b1_y1))\n",
        "        box2_area = abs((b2_x2 - b2_x1) * (b2_y2 - b2_y1))\n",
        "\n",
        "        return inter_area / (box1_area + box2_area - inter_area + 1e-6)\n",
        "\n",
        "    def forward(self, predictions, target):\n",
        "        # 두 박스 중 누가 책임질지 결정\n",
        "        iou_b1 = self.compute_iou(predictions[..., 21:25], target[..., 21:25])\n",
        "        iou_b2 = self.compute_iou(predictions[..., 26:30], target[..., 21:25])\n",
        "\n",
        "        # IoU가 더 큰 박스를 선택\n",
        "        # best_box 값이 0이면 Box1, 1이면 Box2를 따름\n",
        "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
        "        best_box = torch.argmax(ious, dim=0) # (N, 7, 7)\n",
        "\n",
        "        exists_box = target[..., 20].unsqueeze(3) # (N, 7, 7, 1)\n",
        "\n",
        "        # best_box에 따라 예측값 선택\n",
        "        box_predictions = exists_box * (\n",
        "            (best_box.unsqueeze(3) == 0) * predictions[..., 21:25] +\n",
        "            (best_box.unsqueeze(3) == 1) * predictions[..., 26:30]\n",
        "        )\n",
        "        box_targets = exists_box * target[..., 21:25]\n",
        "\n",
        "        # Width, Height 루트 처리\n",
        "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(torch.abs(box_predictions[..., 2:4] + 1e-6))\n",
        "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
        "\n",
        "        box_loss = self.mse(torch.flatten(box_predictions, end_dim=-2),\n",
        "                            torch.flatten(box_targets, end_dim=-2))\n",
        "\n",
        "        pred_box = (\n",
        "            (best_box.unsqueeze(3) == 0) * predictions[..., 20:21] +\n",
        "            (best_box.unsqueeze(3) == 1) * predictions[..., 25:26]\n",
        "        )\n",
        "        object_loss = self.mse(torch.flatten(exists_box * pred_box),\n",
        "                               torch.flatten(exists_box * target[..., 20:21]))\n",
        "\n",
        "        # 물체 없는 곳에서는 Box1, Box2 둘 다 Loss 계산\n",
        "        no_object_loss = self.mse(\n",
        "            torch.flatten((1 - exists_box) * predictions[..., 20:21], start_dim=1),\n",
        "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1)\n",
        "        )\n",
        "        no_object_loss += self.mse(\n",
        "            torch.flatten((1 - exists_box) * predictions[..., 25:26], start_dim=1),\n",
        "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1)\n",
        "        )\n",
        "\n",
        "        # 클래스 손실률\n",
        "        class_loss = self.mse(\n",
        "            torch.flatten(exists_box * predictions[..., :20], end_dim=-2),\n",
        "            torch.flatten(exists_box * target[..., :20], end_dim=-2)\n",
        "        )\n",
        "\n",
        "        loss = (\n",
        "            self.lambda_coord * box_loss\n",
        "            + object_loss\n",
        "            + self.lambda_noobj * no_object_loss\n",
        "            + class_loss\n",
        "        )\n",
        "        return loss"
      ],
      "metadata": {
        "id": "GsLQ8D1lWLh4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = DarkNet().to(device)\n",
        "model = Yolov1_resnet50().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "loss_fn = YoloLoss()\n",
        "\n",
        "print(\"학습 시작한다\")\n",
        "model.train()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  mean_loss = []\n",
        "\n",
        "  for batch_idx, (x,y) in enumerate(train_loader):\n",
        "    x,y = x.to(device), y.to(device)\n",
        "\n",
        "    out = model(x)\n",
        "    loss = loss_fn(out, y)\n",
        "    mean_loss.append(loss.item())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch_idx % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{EPOCHS}] Batch {batch_idx}/{len(train_loader)} Loss: {loss.item():.4f}\")\n",
        "\n",
        "  print(f\"===> Epoch {epoch+1} 평균 Loss: {sum(mean_loss)/len(mean_loss):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgtfAM82M8wV",
        "outputId": "4f5da04d-ec6f-4a8d-f422-f5c5f6b6571a",
        "collapsed": true
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 시작한다\n",
            "Epoch [1/10] Batch 0/1154 Loss: 247.0426\n",
            "Epoch [1/10] Batch 100/1154 Loss: 102.1081\n",
            "Epoch [1/10] Batch 200/1154 Loss: 54.7044\n",
            "Epoch [1/10] Batch 300/1154 Loss: 55.0788\n",
            "Epoch [1/10] Batch 400/1154 Loss: 23.9167\n",
            "Epoch [1/10] Batch 500/1154 Loss: 54.4183\n",
            "Epoch [1/10] Batch 600/1154 Loss: 60.2700\n",
            "Epoch [1/10] Batch 700/1154 Loss: 53.6127\n",
            "Epoch [1/10] Batch 800/1154 Loss: 72.7721\n",
            "Epoch [1/10] Batch 900/1154 Loss: 57.6377\n",
            "Epoch [1/10] Batch 1000/1154 Loss: 140.0523\n",
            "Epoch [1/10] Batch 1100/1154 Loss: 67.4052\n",
            "===> Epoch 1 평균 Loss: 79.7244\n",
            "Epoch [2/10] Batch 0/1154 Loss: 993.0522\n",
            "Epoch [2/10] Batch 100/1154 Loss: 79.2010\n",
            "Epoch [2/10] Batch 200/1154 Loss: 150.7109\n",
            "Epoch [2/10] Batch 300/1154 Loss: 77.4356\n",
            "Epoch [2/10] Batch 400/1154 Loss: 126.7852\n",
            "Epoch [2/10] Batch 500/1154 Loss: 78.3755\n",
            "Epoch [2/10] Batch 600/1154 Loss: 95.9337\n",
            "Epoch [2/10] Batch 700/1154 Loss: 40.9933\n",
            "Epoch [2/10] Batch 800/1154 Loss: 100.9630\n",
            "Epoch [2/10] Batch 900/1154 Loss: 53.9005\n",
            "Epoch [2/10] Batch 1000/1154 Loss: 58.3693\n",
            "Epoch [2/10] Batch 1100/1154 Loss: 48.0734\n",
            "===> Epoch 2 평균 Loss: 84.8122\n",
            "Epoch [3/10] Batch 0/1154 Loss: 53.3107\n",
            "Epoch [3/10] Batch 100/1154 Loss: 63.7641\n",
            "Epoch [3/10] Batch 200/1154 Loss: 35.4637\n",
            "Epoch [3/10] Batch 300/1154 Loss: 40.8482\n",
            "Epoch [3/10] Batch 400/1154 Loss: 28.5360\n",
            "Epoch [3/10] Batch 500/1154 Loss: 90.2449\n",
            "Epoch [3/10] Batch 600/1154 Loss: 670.0579\n",
            "Epoch [3/10] Batch 700/1154 Loss: 85.6319\n",
            "Epoch [3/10] Batch 800/1154 Loss: 75.0830\n",
            "Epoch [3/10] Batch 900/1154 Loss: 423.1364\n",
            "Epoch [3/10] Batch 1000/1154 Loss: 60.1308\n",
            "Epoch [3/10] Batch 1100/1154 Loss: 94.7892\n",
            "===> Epoch 3 평균 Loss: 78.4579\n",
            "Epoch [4/10] Batch 0/1154 Loss: 71.4393\n",
            "Epoch [4/10] Batch 100/1154 Loss: 97.5355\n",
            "Epoch [4/10] Batch 200/1154 Loss: 85.2723\n",
            "Epoch [4/10] Batch 300/1154 Loss: 52.5334\n",
            "Epoch [4/10] Batch 400/1154 Loss: 26.7895\n",
            "Epoch [4/10] Batch 500/1154 Loss: 39.9270\n",
            "Epoch [4/10] Batch 600/1154 Loss: 60.7371\n",
            "Epoch [4/10] Batch 700/1154 Loss: 46.9857\n",
            "Epoch [4/10] Batch 800/1154 Loss: 70.8919\n",
            "Epoch [4/10] Batch 900/1154 Loss: 43.5549\n",
            "Epoch [4/10] Batch 1000/1154 Loss: 51.2360\n",
            "Epoch [4/10] Batch 1100/1154 Loss: 70.7407\n",
            "===> Epoch 4 평균 Loss: 70.1434\n",
            "Epoch [5/10] Batch 0/1154 Loss: 73.1673\n",
            "Epoch [5/10] Batch 100/1154 Loss: 140.1472\n",
            "Epoch [5/10] Batch 200/1154 Loss: 88.4254\n",
            "Epoch [5/10] Batch 300/1154 Loss: 72.2311\n",
            "Epoch [5/10] Batch 400/1154 Loss: 89.1070\n",
            "Epoch [5/10] Batch 500/1154 Loss: 31.2962\n",
            "Epoch [5/10] Batch 600/1154 Loss: 75.6077\n",
            "Epoch [5/10] Batch 700/1154 Loss: 47.5491\n",
            "Epoch [5/10] Batch 800/1154 Loss: 37.1847\n",
            "Epoch [5/10] Batch 900/1154 Loss: 87.0113\n",
            "Epoch [5/10] Batch 1000/1154 Loss: 75.3716\n",
            "Epoch [5/10] Batch 1100/1154 Loss: 73.3622\n",
            "===> Epoch 5 평균 Loss: 80.9910\n",
            "Epoch [6/10] Batch 0/1154 Loss: 66.0499\n",
            "Epoch [6/10] Batch 100/1154 Loss: 55.7068\n",
            "Epoch [6/10] Batch 200/1154 Loss: 49.3502\n",
            "Epoch [6/10] Batch 300/1154 Loss: 54.3226\n",
            "Epoch [6/10] Batch 400/1154 Loss: 65.5003\n",
            "Epoch [6/10] Batch 500/1154 Loss: 71.5214\n",
            "Epoch [6/10] Batch 600/1154 Loss: 39.4846\n",
            "Epoch [6/10] Batch 700/1154 Loss: 42.5191\n",
            "Epoch [6/10] Batch 800/1154 Loss: 80.0041\n",
            "Epoch [6/10] Batch 900/1154 Loss: 82.5214\n",
            "Epoch [6/10] Batch 1000/1154 Loss: 68.3601\n",
            "Epoch [6/10] Batch 1100/1154 Loss: 103.5355\n",
            "===> Epoch 6 평균 Loss: 66.7562\n",
            "Epoch [7/10] Batch 0/1154 Loss: 48.5680\n",
            "Epoch [7/10] Batch 100/1154 Loss: 72.6841\n",
            "Epoch [7/10] Batch 200/1154 Loss: 49.0467\n",
            "Epoch [7/10] Batch 300/1154 Loss: 37.9135\n",
            "Epoch [7/10] Batch 400/1154 Loss: 51.7942\n",
            "Epoch [7/10] Batch 500/1154 Loss: 71.7759\n",
            "Epoch [7/10] Batch 600/1154 Loss: 79.9581\n",
            "Epoch [7/10] Batch 700/1154 Loss: 54.6728\n",
            "Epoch [7/10] Batch 800/1154 Loss: 71.2273\n",
            "Epoch [7/10] Batch 900/1154 Loss: 80.2583\n",
            "Epoch [7/10] Batch 1000/1154 Loss: 63.3593\n",
            "Epoch [7/10] Batch 1100/1154 Loss: 34.5004\n",
            "===> Epoch 7 평균 Loss: 70.9990\n",
            "Epoch [8/10] Batch 0/1154 Loss: 55.2362\n",
            "Epoch [8/10] Batch 100/1154 Loss: 95.1764\n",
            "Epoch [8/10] Batch 200/1154 Loss: 122.1221\n",
            "Epoch [8/10] Batch 300/1154 Loss: 48.5252\n",
            "Epoch [8/10] Batch 400/1154 Loss: 49.7088\n",
            "Epoch [8/10] Batch 500/1154 Loss: 38.5679\n",
            "Epoch [8/10] Batch 600/1154 Loss: 32.6427\n",
            "Epoch [8/10] Batch 700/1154 Loss: 183.4695\n",
            "Epoch [8/10] Batch 800/1154 Loss: 63.7773\n",
            "Epoch [8/10] Batch 900/1154 Loss: 66.1316\n",
            "Epoch [8/10] Batch 1000/1154 Loss: 50.9932\n",
            "Epoch [8/10] Batch 1100/1154 Loss: 35.0829\n",
            "===> Epoch 8 평균 Loss: 61.8574\n",
            "Epoch [9/10] Batch 0/1154 Loss: 69.1308\n",
            "Epoch [9/10] Batch 100/1154 Loss: 41.7904\n",
            "Epoch [9/10] Batch 200/1154 Loss: 57.3354\n",
            "Epoch [9/10] Batch 300/1154 Loss: 214.5469\n",
            "Epoch [9/10] Batch 400/1154 Loss: 60.8320\n",
            "Epoch [9/10] Batch 500/1154 Loss: 32.9626\n",
            "Epoch [9/10] Batch 600/1154 Loss: 63.2265\n",
            "Epoch [9/10] Batch 700/1154 Loss: 93.1593\n",
            "Epoch [9/10] Batch 800/1154 Loss: 57.6452\n",
            "Epoch [9/10] Batch 900/1154 Loss: 66.7555\n",
            "Epoch [9/10] Batch 1000/1154 Loss: 62.2549\n",
            "Epoch [9/10] Batch 1100/1154 Loss: 55.4059\n",
            "===> Epoch 9 평균 Loss: 62.3613\n",
            "Epoch [10/10] Batch 0/1154 Loss: 91.3094\n",
            "Epoch [10/10] Batch 100/1154 Loss: 167.2482\n",
            "Epoch [10/10] Batch 200/1154 Loss: 64.0602\n",
            "Epoch [10/10] Batch 300/1154 Loss: 57.9015\n",
            "Epoch [10/10] Batch 400/1154 Loss: 45.2408\n",
            "Epoch [10/10] Batch 500/1154 Loss: 53.4141\n",
            "Epoch [10/10] Batch 600/1154 Loss: 84.9641\n",
            "Epoch [10/10] Batch 700/1154 Loss: 55.4415\n",
            "Epoch [10/10] Batch 800/1154 Loss: 124.4810\n",
            "Epoch [10/10] Batch 900/1154 Loss: 26.4388\n",
            "Epoch [10/10] Batch 1000/1154 Loss: 51.7824\n",
            "Epoch [10/10] Batch 1100/1154 Loss: 64.8598\n",
            "===> Epoch 10 평균 Loss: 60.6335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장\n",
        "torch.save(model.state_dict(), \"yolo_resnet50_voc.pth\")\n",
        "print(\"모델 저장 완료\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9PXtU7y2EfA",
        "outputId": "d3705e8d-8481-423f-803f-edfb5553fe73"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델 저장 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from collections import Counter\n",
        "\n",
        "# 1. IoU 계산 (Intersection Over Union)\n",
        "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
        "    # boxes_preds shape: (N, 4) -> (x, y, w, h)\n",
        "    # boxes_labels shape: (N, 4)\n",
        "\n",
        "    if box_format == \"midpoint\":\n",
        "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
        "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
        "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
        "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
        "\n",
        "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
        "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
        "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
        "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
        "\n",
        "    x1 = torch.max(box1_x1, box2_x1)\n",
        "    y1 = torch.max(box1_y1, box2_y1)\n",
        "    x2 = torch.min(box1_x2, box2_x2)\n",
        "    y2 = torch.min(box1_y2, box2_y2)\n",
        "\n",
        "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
        "\n",
        "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
        "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
        "\n",
        "    return intersection / (box1_area + box2_area - intersection + 1e-6)\n",
        "\n",
        "# 2. NMS (Non-Maximum Suppression) - 중복 박스 제거\n",
        "def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"midpoint\"):\n",
        "    # bboxes: [[class, score, x, y, w, h], ...]\n",
        "    assert type(bboxes) == list\n",
        "\n",
        "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
        "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
        "    bboxes_after_nms = []\n",
        "\n",
        "    while bboxes:\n",
        "        chosen_box = bboxes.pop(0)\n",
        "\n",
        "        bboxes = [\n",
        "            box for box in bboxes\n",
        "            if box[0] != chosen_box[0]\n",
        "            or intersection_over_union(\n",
        "                torch.tensor(chosen_box[2:]),\n",
        "                torch.tensor(box[2:]),\n",
        "                box_format=box_format\n",
        "            ) < iou_threshold\n",
        "        ]\n",
        "\n",
        "        bboxes_after_nms.append(chosen_box)\n",
        "\n",
        "    return bboxes_after_nms\n",
        "\n",
        "# 3. mAP 계산 (Mean Average Precision)\n",
        "def mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20):\n",
        "    # pred_boxes: [[train_idx, class_pred, prob_score, x1, y1, x2, y2], ...]\n",
        "    average_precisions = []\n",
        "    epsilon = 1e-6\n",
        "\n",
        "    for c in range(num_classes):\n",
        "        detections = []\n",
        "        ground_truths = []\n",
        "\n",
        "        for detection in pred_boxes:\n",
        "            if detection[1] == c:\n",
        "                detections.append(detection)\n",
        "\n",
        "        for true_box in true_boxes:\n",
        "            if true_box[1] == c:\n",
        "                ground_truths.append(true_box)\n",
        "\n",
        "        # 이미지별 정답 개수 카운트 (예: img 0에 강아지 2마리, img 1에 1마리...)\n",
        "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
        "        for key, val in amount_bboxes.items():\n",
        "            amount_bboxes[key] = torch.zeros(val)\n",
        "\n",
        "        detections.sort(key=lambda x: x[2], reverse=True)\n",
        "        TP = torch.zeros(len(detections))\n",
        "        FP = torch.zeros(len(detections))\n",
        "        total_true_bboxes = len(ground_truths)\n",
        "\n",
        "        if total_true_bboxes == 0:\n",
        "            continue\n",
        "\n",
        "        for detection_idx, detection in enumerate(detections):\n",
        "            ground_truth_img = [bbox for bbox in ground_truths if bbox[0] == detection[0]]\n",
        "\n",
        "            num_gts = len(ground_truth_img)\n",
        "            best_iou = 0\n",
        "            best_gt_idx = -1\n",
        "\n",
        "            for idx, gt in enumerate(ground_truth_img):\n",
        "                iou = intersection_over_union(\n",
        "                    torch.tensor(detection[3:]),\n",
        "                    torch.tensor(gt[3:]),\n",
        "                    box_format=box_format,\n",
        "                )\n",
        "\n",
        "                if iou > best_iou:\n",
        "                    best_iou = iou\n",
        "                    best_gt_idx = idx\n",
        "\n",
        "            if best_iou > iou_threshold:\n",
        "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
        "                    TP[detection_idx] = 1\n",
        "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
        "                else:\n",
        "                    FP[detection_idx] = 1\n",
        "            else:\n",
        "                FP[detection_idx] = 1\n",
        "\n",
        "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
        "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
        "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
        "        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n",
        "\n",
        "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
        "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
        "\n",
        "        # 적분 (Area under curve)\n",
        "        average_precisions.append(torch.trapz(precisions, recalls))\n",
        "\n",
        "    return sum(average_precisions) / len(average_precisions)\n",
        "\n",
        "# 4. 모델 출력을 박스 리스트로 변환 (get_bboxes)\n",
        "def get_bboxes(loader, model, iou_threshold, threshold, S=7, device=\"cuda\"):\n",
        "    all_pred_boxes = []\n",
        "    all_true_boxes = []\n",
        "\n",
        "    model.eval()\n",
        "    train_idx = 0\n",
        "\n",
        "    for batch_idx, (x, labels) in enumerate(loader):\n",
        "        x = x.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions = model(x)\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        # (N, 7, 7, 30) -> 리스트 형태로 변환\n",
        "\n",
        "        # --- Decode Predictions ---\n",
        "        bboxes = []\n",
        "        # Box 1\n",
        "        box1_conf = predictions[..., 20:21]\n",
        "        box1_coord = predictions[..., 21:25]\n",
        "        # Box 2\n",
        "        box2_conf = predictions[..., 25:26]\n",
        "        box2_coord = predictions[..., 26:30]\n",
        "        # Class\n",
        "        class_probs = predictions[..., :20]\n",
        "        best_class = class_probs.argmax(-1).unsqueeze(-1)\n",
        "\n",
        "        # Cell indices\n",
        "        cell_indices = torch.arange(S).repeat(batch_size, S, 1).unsqueeze(-1).to(device)\n",
        "\n",
        "        # 좌표 복원 (Relative to Image)\n",
        "        for box_conf, box_coord in [(box1_conf, box1_coord), (box2_conf, box2_coord)]:\n",
        "            x_c = (1/S) * (box_coord[..., 0:1] + cell_indices)\n",
        "            y_c = (1/S) * (box_coord[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
        "            w_h = box_coord[..., 2:4]\n",
        "            converted_box = torch.cat((best_class, box_conf, x_c, y_c, w_h), dim=-1) # (N, 7, 7, 6)\n",
        "\n",
        "            # 배치별로 쪼개기\n",
        "            for idx in range(batch_size):\n",
        "                # 7x7=49개 박스 중 threshold 넘는 것만 NMS 수행\n",
        "                boxes_in_img = converted_box[idx].reshape(-1, 6).tolist()\n",
        "                nms_boxes = non_max_suppression(boxes_in_img, iou_threshold, threshold)\n",
        "                for box in nms_boxes:\n",
        "                    all_pred_boxes.append([train_idx + idx] + box)\n",
        "\n",
        "        # --- Decode Targets ---\n",
        "        # 정답 데이터도 리스트로 변환\n",
        "        for idx in range(batch_size):\n",
        "            label = labels[idx] # (7, 7, 30)\n",
        "            # 물체 있는 곳만 가져오기\n",
        "            mask = label[..., 20] == 1\n",
        "            true_locs = label[mask] # (num_obj, 30)\n",
        "\n",
        "            for true_box in true_locs:\n",
        "                cls = true_box[:20].argmax().item()\n",
        "                x_c = (true_box[21] + mask.nonzero()[0][1]) / S # j + x\n",
        "                y_c = (true_box[22] + mask.nonzero()[0][0]) / S # i + y\n",
        "                w = true_box[23]\n",
        "                h = true_box[24]\n",
        "                all_true_boxes.append([train_idx + idx, cls, 1, x_c.item(), y_c.item(), w.item(), h.item()])\n",
        "\n",
        "        train_idx += batch_size\n",
        "\n",
        "    model.train()\n",
        "    return all_pred_boxes, all_true_boxes"
      ],
      "metadata": {
        "id": "YNy8d6hH2ki_"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"정확도(mAP) 계산 중... (시간이 좀 걸립니다)\")\n",
        "\n",
        "# 1. 모든 예측값과 정답을 리스트로 변환\n",
        "# threshold=0.05: 확신 5% 미만은 버림\n",
        "# iou_threshold=0.5: NMS할 때 겹치는 박스 제거 기준\n",
        "pred_boxes, true_boxes = get_bboxes(test_loader, model, iou_threshold=0.5, threshold=0.05, device=device)\n",
        "\n",
        "# 2. mAP 계산 (IoU 0.5 기준)\n",
        "mAP = mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5, num_classes=20)\n",
        "\n",
        "print(f\"========================================\")\n",
        "print(f\"최종 모델 성능 (mAP @ IoU 0.5): {mAP.item()*100:.2f}%\")\n",
        "print(f\"========================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyU8F_Qx255Q",
        "outputId": "5da2cab3-4435-42f4-8e28-3b78ed97fc19"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정확도(mAP) 계산 중... (시간이 좀 걸립니다)\n",
            "========================================\n",
            "최종 모델 성능 (mAP @ IoU 0.5): 0.61%\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습은 일단 정상적으로 되는 것 같음.\n",
        "\n",
        "근데 resnet을 몸체 갈아 끼우고, 에폭 10 하는데 1시간 반 걸렸는데 loss가 60인거 보고\n",
        "음 얘부터는 노트북 레벨로는 할 수 없구나를 느낌.\n",
        "\n",
        "학습 되는거만 확인하고 여기까지 하자..."
      ],
      "metadata": {
        "id": "GUj9f-mjtiWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "피치 못할 사정으로 모델 정확도 검증 코드는 무지성 제미나이 갈김. 약간 찔린다.\n",
        "\n",
        "근데 아무리 그래도 정확도 0.61%은 많이많이많이 뭔가긴 해.\n",
        "\n",
        "이게 노트북의 한계다. 나중에 컴으로 학습 50에폭 때리고 해보자"
      ],
      "metadata": {
        "id": "j2cO4-SY3QwW"
      }
    }
  ]
}