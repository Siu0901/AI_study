{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmTWvYCw6I3PbthUT6eOl4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siu0901/AI_study/blob/main/%EB%AC%B8%EC%9E%90_%EB%8B%A8%EC%9C%84_RNN_%EA%B3%B5%EB%B6%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iyV0Dj5-1jAk"
      },
      "outputs": [],
      "source": [
        "# 지금까진 전부 입출력 단위가 단어 벡터였다면 예는 문자임\n",
        "# appl 이 있으면 e 가 들어가야 apple 이 된다 이런식으로 예측하는 거임"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자 단위 RNN 언어 (다 대 다)"
      ],
      "metadata": {
        "id": "jyWQ9tnA15RU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터에 대한 이해와 전처리\n",
        "# import numpy as np\n",
        "# import urllib.request\n",
        "# from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# urllib.request.urlretrieve(\"http://www.gutenberg.org/files/11/11-0.txt\", filename=\"11-0.txt\")\n",
        "\n",
        "# f = open('11-0.txt', 'rb')\n",
        "# sentences = []\n",
        "# for sentence in f:\n",
        "#   sentence =sentence.strip()\n",
        "#   sentence = sentence.lower()\n",
        "#   sentence = sentence.decode('ascii', 'ignore')\n",
        "#   # \\xe2\\x80\\x99 등과 같은 바이트 열 제거\n",
        "#   if len(sentence) > 0:\n",
        "#     sentences.append(sentence)\n",
        "# f.close()\n",
        "\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 데이터 로드\n",
        "urllib.request.urlretrieve(\"http://www.gutenberg.org/files/11/11-0.txt\", filename=\"11-0.txt\")\n",
        "\n",
        "f = open('11-0.txt', 'rb')\n",
        "sentences = []\n",
        "for sentence in f: # 데이터로부터 한 줄씩 읽는다.\n",
        "    sentence = sentence.strip() # strip()을 통해 \\r, \\n을 제거한다.\n",
        "    sentence = sentence.lower() # 소문자화.\n",
        "    sentence = sentence.decode('ascii', 'ignore') # \\xe2\\x80\\x99 등과 같은 바이트 열 제거\n",
        "    if len(sentence) > 0:\n",
        "        sentences.append(sentence)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "FYVpXOqC2F_2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TumyGMW4DHp",
        "outputId": "ab3d157e-bb85-4ce4-a70a-bfc7e43ea68c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['*** start of the project gutenberg ebook 11 ***',\n",
              " '[illustration]',\n",
              " 'alices adventures in wonderland',\n",
              " 'by lewis carroll',\n",
              " 'the millennium fulcrum edition 3.0',\n",
              " 'contents',\n",
              " 'chapter i.     down the rabbit-hole',\n",
              " 'chapter ii.    the pool of tears',\n",
              " 'chapter iii.   a caucus-race and a long tale',\n",
              " 'chapter iv.    the rabbit sends in a little bill']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_data = ' '.join(sentences[20:])\n",
        "print('문자 데이터 수:',len(total_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dl9t_v3PbriC",
        "outputId": "0d0652e6-9e34-469f-da3a-e87d7f003fc6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 데이터 수: 139626\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(total_data[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9MWmn-Tbws_",
        "outputId": "c90881ac-23ee-43ad-ac9e-cd9a78b01302"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, and what is the use of a book, thought alice without pictures or conversations? so she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting u\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_vocab = sorted(list(set(total_data)))\n",
        "vocab_size = len(char_vocab)\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJ4F-HWZcZfZ",
        "outputId": "c363b54a-6449-4270-99e6-c928306261ab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자에 고유한 정수 부여\n",
        "char_to_index= dict((char, index) for index, char in enumerate(char_vocab))\n",
        "print(char_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4O1eCLedJFb",
        "outputId": "25360ad1-c3ec-45b5-db43-402259bd5317"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ': 0, '!': 1, '(': 2, ')': 3, '*': 4, ',': 5, '-': 6, '.': 7, '1': 8, ':': 9, ';': 10, '?': 11, '[': 12, ']': 13, '_': 14, 'a': 15, 'b': 16, 'c': 17, 'd': 18, 'e': 19, 'f': 20, 'g': 21, 'h': 22, 'i': 23, 'j': 24, 'k': 25, 'l': 26, 'm': 27, 'n': 28, 'o': 29, 'p': 30, 'q': 31, 'r': 32, 's': 33, 't': 34, 'u': 35, 'v': 36, 'w': 37, 'x': 38, 'y': 39, 'z': 40}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 정수로부터 문자를 리턴하는 딕셔너리\n",
        "index_to_char = {}\n",
        "for key, value in char_to_index.items():\n",
        "    index_to_char[value] = key"
      ],
      "metadata": {
        "id": "NTEr-AyEdb4q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# appl (입력 시퀀스) -> pple (예측해야하는 시퀀스)\n",
        "train_X = 'appl'\n",
        "train_y = 'pple'"
      ],
      "metadata": {
        "id": "U7QJ5NZAkkTv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터를 만드는 방법은 문장 샘플의 길이를 정하고,\n",
        "# 해당 길이만큼 문자열 전체를 등분하는 것"
      ],
      "metadata": {
        "id": "CJwP3PdolSap"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 60\n",
        "n_samples = int(np.floor((len(total_data) - 1) / seq_length))\n",
        "print ('샘플의 수 : {}'.format(n_samples))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApZm4_aVlWnj",
        "outputId": "bfe449ec-5f6a-4870-e39f-88e2cd50d434"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "샘플의 수 : 2327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = []\n",
        "train_y = []\n",
        "\n",
        "for i in range(n_samples):\n",
        "  # 0:60 -> 60:120 -> 120:180로 loop를 돌면서 문장 샘플을 1개씩 pick.\n",
        "  X_sample = total_data[i * seq_length: (i+1) * seq_length]\n",
        "\n",
        "  # 정수 인코딩\n",
        "  X_encoded = [char_to_index[c] for c in X_sample]\n",
        "  train_X.append(X_encoded)\n",
        "\n",
        "  # 오른쪽으로 1칸 쉬프트\n",
        "  y_sample = total_data[i * seq_length + 1: (i+1) * seq_length + 1]\n",
        "  y_encoded = [char_to_index[c] for c in y_sample]\n",
        "  train_y.append(y_encoded)"
      ],
      "metadata": {
        "id": "_gIU9uF0ljom"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_y[0]은 train_X[0]에서 오른쪽으로 한 칸 쉬프트 된 문장임을 알 수 있다\n",
        "print('X 데이터의 첫번째 샘플 :',train_X[0])\n",
        "print('y 데이터의 첫번째 샘플 :',train_y[0])\n",
        "print('-'*50)\n",
        "print('X 데이터의 첫번째 샘플 디코딩 :',[index_to_char[i] for i in train_X[0]])\n",
        "print('y 데이터의 첫번째 샘플 디코딩 :',[index_to_char[i] for i in train_y[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGej7ngqnoIN",
        "outputId": "c9479ec7-2536-4853-bd62-f3260c64fef6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X 데이터의 첫번째 샘플 : [15, 26, 23, 17, 19, 0, 37, 15, 33, 0, 16, 19, 21, 23, 28, 28, 23, 28, 21, 0, 34, 29, 0, 21, 19, 34, 0, 36, 19, 32, 39, 0, 34, 23, 32, 19, 18, 0, 29, 20, 0, 33, 23, 34, 34, 23, 28, 21, 0, 16, 39, 0, 22, 19, 32, 0, 33, 23, 33, 34]\n",
            "y 데이터의 첫번째 샘플 : [26, 23, 17, 19, 0, 37, 15, 33, 0, 16, 19, 21, 23, 28, 28, 23, 28, 21, 0, 34, 29, 0, 21, 19, 34, 0, 36, 19, 32, 39, 0, 34, 23, 32, 19, 18, 0, 29, 20, 0, 33, 23, 34, 34, 23, 28, 21, 0, 16, 39, 0, 22, 19, 32, 0, 33, 23, 33, 34, 19]\n",
            "--------------------------------------------------\n",
            "X 데이터의 첫번째 샘플 디코딩 : ['a', 'l', 'i', 'c', 'e', ' ', 'w', 'a', 's', ' ', 'b', 'e', 'g', 'i', 'n', 'n', 'i', 'n', 'g', ' ', 't', 'o', ' ', 'g', 'e', 't', ' ', 'v', 'e', 'r', 'y', ' ', 't', 'i', 'r', 'e', 'd', ' ', 'o', 'f', ' ', 's', 'i', 't', 't', 'i', 'n', 'g', ' ', 'b', 'y', ' ', 'h', 'e', 'r', ' ', 's', 'i', 's', 't']\n",
            "y 데이터의 첫번째 샘플 디코딩 : ['l', 'i', 'c', 'e', ' ', 'w', 'a', 's', ' ', 'b', 'e', 'g', 'i', 'n', 'n', 'i', 'n', 'g', ' ', 't', 'o', ' ', 'g', 'e', 't', ' ', 'v', 'e', 'r', 'y', ' ', 't', 'i', 'r', 'e', 'd', ' ', 'o', 'f', ' ', 's', 'i', 't', 't', 'i', 'n', 'g', ' ', 'b', 'y', ' ', 'h', 'e', 'r', ' ', 's', 'i', 's', 't', 'e']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_X[1])\n",
        "print(train_y[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiLO8UbUoNpO",
        "outputId": "4c11922d-28b1-43d9-a9cb-473c6360d5ef"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[19, 32, 0, 29, 28, 0, 34, 22, 19, 0, 16, 15, 28, 25, 5, 0, 15, 28, 18, 0, 29, 20, 0, 22, 15, 36, 23, 28, 21, 0, 28, 29, 34, 22, 23, 28, 21, 0, 34, 29, 0, 18, 29, 9, 0, 29, 28, 17, 19, 0, 29, 32, 0, 34, 37, 23, 17, 19, 0, 33]\n",
            "[32, 0, 29, 28, 0, 34, 22, 19, 0, 16, 15, 28, 25, 5, 0, 15, 28, 18, 0, 29, 20, 0, 22, 15, 36, 23, 28, 21, 0, 28, 29, 34, 22, 23, 28, 21, 0, 34, 29, 0, 18, 29, 9, 0, 29, 28, 17, 19, 0, 29, 32, 0, 34, 37, 23, 17, 19, 0, 33, 22]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자 단위 RNN에서는 입력 시퀀스에 대해서 워드 임베딩을 하지 않음\n",
        "# 임베딩층을 사용하지 않기에 train_X에 대해 워드 임베딩 함"
      ],
      "metadata": {
        "id": "uRvI7mFaoWq7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = to_categorical(train_X)\n",
        "train_y = to_categorical(train_y)\n",
        "\n",
        "print('train_X의 크기(shape) : {}'.format(train_X.shape)) # 원-핫 인코딩\n",
        "print('train_y의 크기(shape) : {}'.format(train_y.shape)) # 원-핫 인코딩"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lXDi4UKohNZ",
        "outputId": "12d26f61-bc7d-482d-a822-6bcd870f74b0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_X의 크기(shape) : (2327, 60, 41)\n",
            "train_y의 크기(shape) : (2327, 60, 41)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfdbe2Jfomt4",
        "outputId": "83dc97f7-75b2-49e8-93d1-4b4982195cd1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 샘플 수 2327개, 입력 시퀀스의 길이 60, 각 벡터의 차원이 41임을 의미함"
      ],
      "metadata": {
        "id": "hTwnyHMjqCGd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rnn_image6between7.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "yyVhZz-8pvCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 설계하기\n",
        "\n",
        "# 모델은 다 대 다 구조의 LSTM을 사용하며 LSTM 은닉층은 두 개를 사용한다.\n",
        "# 전결합층을 출력층으로 문자 집합 크기만큼의 뉴런을 배치하여 모델을 설계한다.\n",
        "\n",
        "# 해당 모델은 모든 시점에서 모든 가능한 문자 중 하나의 문자를 예측하는\n",
        "# 다중 클래스 분류 문제를 수행하는 모델"
      ],
      "metadata": {
        "id": "bAN9rTLXqKTn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, TimeDistributed\n",
        "\n",
        "hidden_units = 256\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(hidden_units, input_shape=(None, train_X.shape[2]),return_sequences=True))\n",
        "model.add(LSTM(hidden_units, return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(vocab_size, activation='softmax')))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(train_X, train_y, epochs=80, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fq8vPhzlqOoh",
        "outputId": "1efa9ba2-f076-45dc-b35e-9d6e64ce7d58"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "73/73 - 40s - 543ms/step - accuracy: 0.1846 - loss: 3.0282\n",
            "Epoch 2/80\n",
            "73/73 - 31s - 427ms/step - accuracy: 0.2505 - loss: 2.6921\n",
            "Epoch 3/80\n",
            "73/73 - 32s - 442ms/step - accuracy: 0.3386 - loss: 2.3430\n",
            "Epoch 4/80\n",
            "73/73 - 40s - 546ms/step - accuracy: 0.3770 - loss: 2.1810\n",
            "Epoch 5/80\n",
            "73/73 - 41s - 563ms/step - accuracy: 0.4063 - loss: 2.0681\n",
            "Epoch 6/80\n",
            "73/73 - 32s - 444ms/step - accuracy: 0.4299 - loss: 1.9730\n",
            "Epoch 7/80\n",
            "73/73 - 40s - 546ms/step - accuracy: 0.4493 - loss: 1.8957\n",
            "Epoch 8/80\n",
            "73/73 - 42s - 574ms/step - accuracy: 0.4680 - loss: 1.8301\n",
            "Epoch 9/80\n",
            "73/73 - 34s - 463ms/step - accuracy: 0.4845 - loss: 1.7716\n",
            "Epoch 10/80\n",
            "73/73 - 39s - 534ms/step - accuracy: 0.4983 - loss: 1.7188\n",
            "Epoch 11/80\n",
            "73/73 - 33s - 445ms/step - accuracy: 0.5097 - loss: 1.6724\n",
            "Epoch 12/80\n",
            "73/73 - 31s - 428ms/step - accuracy: 0.5213 - loss: 1.6264\n",
            "Epoch 13/80\n",
            "73/73 - 32s - 443ms/step - accuracy: 0.5307 - loss: 1.5850\n",
            "Epoch 14/80\n",
            "73/73 - 33s - 448ms/step - accuracy: 0.5411 - loss: 1.5463\n",
            "Epoch 15/80\n",
            "73/73 - 41s - 559ms/step - accuracy: 0.5521 - loss: 1.5078\n",
            "Epoch 16/80\n",
            "73/73 - 32s - 433ms/step - accuracy: 0.5630 - loss: 1.4707\n",
            "Epoch 17/80\n",
            "73/73 - 32s - 445ms/step - accuracy: 0.5706 - loss: 1.4390\n",
            "Epoch 18/80\n",
            "73/73 - 33s - 445ms/step - accuracy: 0.5799 - loss: 1.4061\n",
            "Epoch 19/80\n",
            "73/73 - 41s - 565ms/step - accuracy: 0.5877 - loss: 1.3769\n",
            "Epoch 20/80\n",
            "73/73 - 32s - 433ms/step - accuracy: 0.5973 - loss: 1.3445\n",
            "Epoch 21/80\n",
            "73/73 - 33s - 445ms/step - accuracy: 0.6053 - loss: 1.3148\n",
            "Epoch 22/80\n",
            "73/73 - 31s - 429ms/step - accuracy: 0.6153 - loss: 1.2827\n",
            "Epoch 23/80\n",
            "73/73 - 33s - 451ms/step - accuracy: 0.6230 - loss: 1.2533\n",
            "Epoch 24/80\n",
            "73/73 - 41s - 561ms/step - accuracy: 0.6315 - loss: 1.2234\n",
            "Epoch 25/80\n",
            "73/73 - 32s - 433ms/step - accuracy: 0.6411 - loss: 1.1926\n",
            "Epoch 26/80\n",
            "73/73 - 33s - 454ms/step - accuracy: 0.6497 - loss: 1.1626\n",
            "Epoch 27/80\n",
            "73/73 - 33s - 453ms/step - accuracy: 0.6596 - loss: 1.1319\n",
            "Epoch 28/80\n",
            "73/73 - 32s - 435ms/step - accuracy: 0.6673 - loss: 1.1037\n",
            "Epoch 29/80\n",
            "73/73 - 42s - 573ms/step - accuracy: 0.6790 - loss: 1.0680\n",
            "Epoch 30/80\n",
            "73/73 - 35s - 480ms/step - accuracy: 0.6886 - loss: 1.0372\n",
            "Epoch 31/80\n",
            "73/73 - 32s - 444ms/step - accuracy: 0.6977 - loss: 1.0054\n",
            "Epoch 32/80\n",
            "73/73 - 42s - 569ms/step - accuracy: 0.7087 - loss: 0.9712\n",
            "Epoch 33/80\n",
            "73/73 - 31s - 428ms/step - accuracy: 0.7175 - loss: 0.9408\n",
            "Epoch 34/80\n",
            "73/73 - 42s - 577ms/step - accuracy: 0.7288 - loss: 0.9068\n",
            "Epoch 35/80\n",
            "73/73 - 32s - 433ms/step - accuracy: 0.7400 - loss: 0.8737\n",
            "Epoch 36/80\n",
            "73/73 - 34s - 459ms/step - accuracy: 0.7497 - loss: 0.8450\n",
            "Epoch 37/80\n",
            "73/73 - 31s - 431ms/step - accuracy: 0.7603 - loss: 0.8110\n",
            "Epoch 38/80\n",
            "73/73 - 33s - 448ms/step - accuracy: 0.7702 - loss: 0.7802\n",
            "Epoch 39/80\n",
            "73/73 - 31s - 431ms/step - accuracy: 0.7796 - loss: 0.7503\n",
            "Epoch 40/80\n",
            "73/73 - 32s - 440ms/step - accuracy: 0.7908 - loss: 0.7187\n",
            "Epoch 41/80\n",
            "73/73 - 31s - 431ms/step - accuracy: 0.7990 - loss: 0.6923\n",
            "Epoch 42/80\n",
            "73/73 - 32s - 440ms/step - accuracy: 0.8101 - loss: 0.6611\n",
            "Epoch 43/80\n",
            "73/73 - 40s - 549ms/step - accuracy: 0.8191 - loss: 0.6334\n",
            "Epoch 44/80\n",
            "73/73 - 32s - 442ms/step - accuracy: 0.8275 - loss: 0.6064\n",
            "Epoch 45/80\n",
            "73/73 - 42s - 574ms/step - accuracy: 0.8349 - loss: 0.5818\n",
            "Epoch 46/80\n",
            "73/73 - 31s - 429ms/step - accuracy: 0.8438 - loss: 0.5559\n",
            "Epoch 47/80\n",
            "73/73 - 32s - 445ms/step - accuracy: 0.8521 - loss: 0.5303\n",
            "Epoch 48/80\n",
            "73/73 - 31s - 427ms/step - accuracy: 0.8597 - loss: 0.5071\n",
            "Epoch 49/80\n",
            "73/73 - 32s - 442ms/step - accuracy: 0.8691 - loss: 0.4794\n",
            "Epoch 50/80\n",
            "73/73 - 32s - 432ms/step - accuracy: 0.8762 - loss: 0.4578\n",
            "Epoch 51/80\n",
            "73/73 - 32s - 441ms/step - accuracy: 0.8838 - loss: 0.4342\n",
            "Epoch 52/80\n",
            "73/73 - 32s - 433ms/step - accuracy: 0.8893 - loss: 0.4176\n",
            "Epoch 53/80\n",
            "73/73 - 32s - 438ms/step - accuracy: 0.8959 - loss: 0.3982\n",
            "Epoch 54/80\n",
            "73/73 - 32s - 432ms/step - accuracy: 0.9034 - loss: 0.3755\n",
            "Epoch 55/80\n",
            "73/73 - 33s - 453ms/step - accuracy: 0.9072 - loss: 0.3614\n",
            "Epoch 56/80\n",
            "73/73 - 33s - 447ms/step - accuracy: 0.9135 - loss: 0.3420\n",
            "Epoch 57/80\n",
            "73/73 - 31s - 430ms/step - accuracy: 0.9179 - loss: 0.3274\n",
            "Epoch 58/80\n",
            "73/73 - 32s - 442ms/step - accuracy: 0.9248 - loss: 0.3078\n",
            "Epoch 59/80\n",
            "73/73 - 31s - 431ms/step - accuracy: 0.9294 - loss: 0.2925\n",
            "Epoch 60/80\n",
            "73/73 - 33s - 449ms/step - accuracy: 0.9333 - loss: 0.2768\n",
            "Epoch 61/80\n",
            "73/73 - 31s - 431ms/step - accuracy: 0.9385 - loss: 0.2618\n",
            "Epoch 62/80\n",
            "73/73 - 33s - 448ms/step - accuracy: 0.9416 - loss: 0.2513\n",
            "Epoch 63/80\n",
            "73/73 - 41s - 559ms/step - accuracy: 0.9421 - loss: 0.2450\n",
            "Epoch 64/80\n",
            "73/73 - 41s - 562ms/step - accuracy: 0.9440 - loss: 0.2366\n",
            "Epoch 65/80\n",
            "73/73 - 32s - 443ms/step - accuracy: 0.9473 - loss: 0.2247\n",
            "Epoch 66/80\n",
            "73/73 - 31s - 427ms/step - accuracy: 0.9501 - loss: 0.2152\n",
            "Epoch 67/80\n",
            "73/73 - 32s - 444ms/step - accuracy: 0.9525 - loss: 0.2051\n",
            "Epoch 68/80\n",
            "73/73 - 41s - 561ms/step - accuracy: 0.9547 - loss: 0.1966\n",
            "Epoch 69/80\n",
            "73/73 - 32s - 439ms/step - accuracy: 0.9563 - loss: 0.1885\n",
            "Epoch 70/80\n",
            "73/73 - 33s - 448ms/step - accuracy: 0.9569 - loss: 0.1838\n",
            "Epoch 71/80\n",
            "73/73 - 32s - 441ms/step - accuracy: 0.9584 - loss: 0.1770\n",
            "Epoch 72/80\n",
            "73/73 - 32s - 442ms/step - accuracy: 0.9601 - loss: 0.1690\n",
            "Epoch 73/80\n",
            "73/73 - 32s - 440ms/step - accuracy: 0.9606 - loss: 0.1650\n",
            "Epoch 74/80\n",
            "73/73 - 33s - 450ms/step - accuracy: 0.9610 - loss: 0.1603\n",
            "Epoch 75/80\n",
            "73/73 - 32s - 444ms/step - accuracy: 0.9613 - loss: 0.1572\n",
            "Epoch 76/80\n",
            "73/73 - 31s - 431ms/step - accuracy: 0.9613 - loss: 0.1552\n",
            "Epoch 77/80\n",
            "73/73 - 33s - 446ms/step - accuracy: 0.9617 - loss: 0.1522\n",
            "Epoch 78/80\n",
            "73/73 - 32s - 432ms/step - accuracy: 0.9614 - loss: 0.1518\n",
            "Epoch 79/80\n",
            "73/73 - 33s - 449ms/step - accuracy: 0.9596 - loss: 0.1568\n",
            "Epoch 80/80\n",
            "73/73 - 32s - 432ms/step - accuracy: 0.9533 - loss: 0.1752\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x796a96ce2db0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_generation(model, length):\n",
        "  # 문자에 대한 랜덤한 정수 생성\n",
        "  ix = [np.random.randint(vocab_size)]\n",
        "\n",
        "  # 랜덤한 정수로부터 맵핑되는 문자 생성\n",
        "  y_char = [index_to_char[ix[-1]]]\n",
        "  print(ix[-1], '번 문자', y_char[-1], '로 예측을 시작!')\n",
        "\n",
        "  # (1, length, 41) 크기의 x 생성. #LSTM의 입력 시퀀스 생성\n",
        "  X = np.zeros((1, length, vocab_size))\n",
        "\n",
        "  for i in range(length):\n",
        "    # X[0][i][예측한 문자의 인덱스] = 1, 즉, 예측 문자를 다음 입력 시퀀스에 추가\n",
        "    X[0][i][ix[-1]] = 1\n",
        "    print(index_to_char[ix[-1]], end=\"\")\n",
        "    ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
        "    y_char.append(index_to_char[ix[-1]])\n",
        "  return (' ').join(y_char)"
      ],
      "metadata": {
        "id": "yGDeBZMfMyR5"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = sentence_generation(model, 100)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxiz7HAMPXlN",
        "outputId": "3584daa3-e087-4941-d497-b570ad669878"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18 번 문자 d 로 예측을 시작!\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "d   t h e   d u c h e s s ,   a s   p i g s   h a v e   t o   f l y ;   a n d   t h e   m   b u t   h e r e ,   t o   a l i c e s   f l o w e r e   _ ,   y o u   k n o w .   s h e   s a i d   t o   h e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ㅅㅂ 진짜 문장이 걍 존나 그럴 듯한 문장을 생성함"
      ],
      "metadata": {
        "id": "UaZzChp_ReyH"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}